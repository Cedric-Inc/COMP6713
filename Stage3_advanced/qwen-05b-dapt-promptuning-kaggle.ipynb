{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11556829,"sourceType":"datasetVersion","datasetId":7237201},{"sourceId":356607,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":297306,"modelId":317915}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:25:48.495445Z","iopub.execute_input":"2025-04-25T08:25:48.496004Z","iopub.status.idle":"2025-04-25T08:25:48.766955Z","shell.execute_reply.started":"2025-04-25T08:25:48.495973Z","shell.execute_reply":"2025-04-25T08:25:48.766191Z"},"id":"Oyp2dgZbGjHu","outputId":"0859691b-aa48-4310-cb2c-2ffe8737ea2d"},"outputs":[{"name":"stdout","text":"/kaggle/input/job-dataset/unlabelled_development_set.csv\n/kaggle/input/job-dataset/no_agument_training_5118.json\n/kaggle/input/job-dataset/agument_work_5316.json\n/kaggle/input/job-dataset/prompts_complete_test_salary.json\n/kaggle/input/job-dataset/work_arrangements_development_set.csv\n/kaggle/input/job-dataset/prompts_complete_test_work.json\n/kaggle/input/job-dataset/work_arrangements_test_set.csv\n/kaggle/input/job-dataset/combined_prompts_complete_6k.json\n/kaggle/input/job-dataset/seniority_labelled_test_set.csv\n/kaggle/input/job-dataset/agument_all_7998.json\n/kaggle/input/job-dataset/agument_training_7800.json\n/kaggle/input/job-dataset/seniority_labelled_development_set.csv\n/kaggle/input/job-dataset/unlabelled_development_set.json\n/kaggle/input/job-dataset/prompts_complete_test_seniority.json\n/kaggle/input/job-dataset/salary_labelled_development_set.csv\n/kaggle/input/job-dataset/test_1355.json\n/kaggle/input/job-dataset/combined_prompts_complete_train_80.json\n/kaggle/input/job-dataset/combined_prompts_complete_val_20.json\n/kaggle/input/job-dataset/salary_labelled_test_set.csv\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/config.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/model.safetensors\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/generation_config.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/config.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/merges.txt\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/trainer_state.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/training_args.bin\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/tokenizer.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/vocab.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/tokenizer_config.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/scheduler.pt\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/model.safetensors\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/special_tokens_map.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/optimizer.pt\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/rng_state.pth\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/added_tokens.json\n/kaggle/input/dapt-model/transformers/default/1/dapt_model/checkpoint-358/generation_config.json\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["Note - Setting of this notebook model\n","\n","LLM model: qwen0.5b\n","\n","Training method: DAPT loading + prompt tuning\n","\n","epoch:4\n","\n","batch:8\n","\n","learning rate: 3e-3\n","\n","prompt_tuning_training set: /kaggle/input/job-dataset/agument_work_5316.json\n","\n"],"metadata":{"id":"HKe1XsS5GjHw"}},{"cell_type":"code","source":["input_path = \"/kaggle/input/job-dataset/agument_work_5316.json\"\n","input_df = pd.read_json(input_path)\n","input_df"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:25:56.825154Z","iopub.execute_input":"2025-04-25T08:25:56.825819Z","iopub.status.idle":"2025-04-25T08:25:57.130062Z","shell.execute_reply.started":"2025-04-25T08:25:56.825792Z","shell.execute_reply":"2025-04-25T08:25:57.129329Z"},"id":"BmwGQ7GgGjHx","outputId":"e404a935-3000-40b6-8f54-bbf4ec59f9cd"},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                 prompt  \\\n0     [TASK: Salary] Extract salary information from...   \n1     [TASK: Salary] Extract salary information from...   \n2     [TASK: Salary] Extract salary information from...   \n3     [TASK: Salary] Extract salary information from...   \n4     [TASK: Salary] Extract salary information from...   \n...                                                 ...   \n5311  [TASK: Work Arrangement] Extract work arrangem...   \n5312  [TASK: Work Arrangement] Extract work arrangem...   \n5313  [TASK: Work Arrangement] Extract work arrangem...   \n5314  [TASK: Work Arrangement] Extract work arrangem...   \n5315  [TASK: Work Arrangement] Extract work arrangem...   \n\n                     complete  \n0     17500-17500-PHP-MONTHLY  \n1     16000-16000-PHP-MONTHLY  \n2               0-0-None-None  \n3               0-0-None-None  \n4               0-0-None-None  \n...                       ...  \n5311                   Remote  \n5312                   Hybrid  \n5313                   Hybrid  \n5314                   Hybrid  \n5315                   Hybrid  \n\n[5316 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>complete</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[TASK: Salary] Extract salary information from...</td>\n      <td>17500-17500-PHP-MONTHLY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[TASK: Salary] Extract salary information from...</td>\n      <td>16000-16000-PHP-MONTHLY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[TASK: Salary] Extract salary information from...</td>\n      <td>0-0-None-None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[TASK: Salary] Extract salary information from...</td>\n      <td>0-0-None-None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[TASK: Salary] Extract salary information from...</td>\n      <td>0-0-None-None</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5311</th>\n      <td>[TASK: Work Arrangement] Extract work arrangem...</td>\n      <td>Remote</td>\n    </tr>\n    <tr>\n      <th>5312</th>\n      <td>[TASK: Work Arrangement] Extract work arrangem...</td>\n      <td>Hybrid</td>\n    </tr>\n    <tr>\n      <th>5313</th>\n      <td>[TASK: Work Arrangement] Extract work arrangem...</td>\n      <td>Hybrid</td>\n    </tr>\n    <tr>\n      <th>5314</th>\n      <td>[TASK: Work Arrangement] Extract work arrangem...</td>\n      <td>Hybrid</td>\n    </tr>\n    <tr>\n      <th>5315</th>\n      <td>[TASK: Work Arrangement] Extract work arrangem...</td>\n      <td>Hybrid</td>\n    </tr>\n  </tbody>\n</table>\n<p>5316 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["import os\n","import time\n","import torch\n","from datasets import load_dataset\n","from transformers import (  # transformer\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling\n",")\n","from peft import (\n","    get_peft_model,\n","    PromptTuningConfig,\n","    TaskType\n",")\n","from peft import PeftModel\n","\n","# 1. Environment Setting\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Setting GPU\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","# 2. Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"Qwen/Qwen2.5-0.5B-Instruct\",\n","    trust_remote_code=True,\n","    use_fast=False,\n","    local_files_only=False\n","    )"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:25:59.964041Z","iopub.execute_input":"2025-04-25T08:25:59.964313Z","iopub.status.idle":"2025-04-25T08:26:31.047541Z","shell.execute_reply.started":"2025-04-25T08:25:59.964290Z","shell.execute_reply":"2025-04-25T08:26:31.046519Z"},"colab":{"referenced_widgets":["95509988b5254f7fa22dc2f27b119fcb","6b8159077e514dc39b9efbd0f6065a2f","ac668efba3f14c9b9cd4ee8802d4f47b","3d07ffad124a44d3a78ac1406b1e5fc5"]},"id":"7LcyBi_-GjHx","outputId":"93d89adc-949d-4ab6-b369-7c8d0872a622"},"outputs":[{"name":"stderr","text":"2025-04-25 08:26:14.958158: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745569575.186679      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745569575.254861      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95509988b5254f7fa22dc2f27b119fcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8159077e514dc39b9efbd0f6065a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac668efba3f14c9b9cd4ee8802d4f47b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d07ffad124a44d3a78ac1406b1e5fc5"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["base_path = \"/kaggle/input/dapt-model/transformers/default/1/dapt_model\"\n","model = AutoModelForCausalLM.from_pretrained(base_path, trust_remote_code=True)\n","\n","# # 5. Check trainable params (only prompt tokens should be trainable now)\n","# model.print_trainable_parameters()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:26:31.048898Z","iopub.execute_input":"2025-04-25T08:26:31.049549Z","iopub.status.idle":"2025-04-25T08:26:32.213803Z","shell.execute_reply.started":"2025-04-25T08:26:31.049526Z","shell.execute_reply":"2025-04-25T08:26:32.213120Z"},"id":"cXzNxpKVGjHx","outputId":"8939d1de-c469-4d5f-b6fb-49eccc72ec5b"},"outputs":[{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["# 3. 添加pad_token（如果不存在）\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# 4. 配置Prompt Tuning\n","peft_config = PromptTuningConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    prompt_tuning_init=\"TEXT\",\n","    prompt_tuning_init_text=\"Extract key information from the text:\",  # 英文初始化文本\n","    num_virtual_tokens=20,\n","    tokenizer_name_or_path='Qwen/Qwen2.5-0.5B-Instruct',\n","    token_dim=model.config.hidden_size,\n","    num_transformer_submodules=1\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:26:35.343778Z","iopub.execute_input":"2025-04-25T08:26:35.344089Z","iopub.status.idle":"2025-04-25T08:26:35.348955Z","shell.execute_reply.started":"2025-04-25T08:26:35.344068Z","shell.execute_reply":"2025-04-25T08:26:35.348121Z"},"id":"f8hZD8JeGjHx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = get_peft_model(model, peft_config).to(device)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:26:37.558761Z","iopub.execute_input":"2025-04-25T08:26:37.559209Z","iopub.status.idle":"2025-04-25T08:26:51.033698Z","shell.execute_reply.started":"2025-04-25T08:26:37.559177Z","shell.execute_reply":"2025-04-25T08:26:51.032905Z"},"id":"P_EEDtOQGjHx"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\n","def load_and_preprocess(data_path):\n","    dataset = load_dataset(\"json\", data_files=data_path)\n","\n","    dataset = dataset.filter(lambda x: len(x[\"prompt\"]) > 0 and len(x[\"complete\"]) > 0)\n","\n","    def tokenize_function(examples):\n","        # print(examples)\n","        completions = [str(c) for c in examples[\"complete\"]]\n","        texts = [p + c + tokenizer.eos_token for p, c in zip(examples[\"prompt\"], completions)]\n","\n","        tokenized = tokenizer(\n","            texts,\n","            max_length=512,\n","            truncation=True,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        )\n","\n","        prompt_tokenized = tokenizer(examples[\"prompt\"], add_special_tokens=False)\n","        prompt_lengths = [len(ids) for ids in prompt_tokenized[\"input_ids\"]]\n","\n","        labels = tokenized[\"input_ids\"].clone()\n","        for i, length in enumerate(prompt_lengths):\n","            if length >= 512:\n","                length = 511\n","            labels[i][:length] = -100\n","\n","        tokenized[\"labels\"] = labels\n","        return tokenized\n","\n","    return dataset.map(tokenize_function, batched=True)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:26:56.232214Z","iopub.execute_input":"2025-04-25T08:26:56.232481Z","iopub.status.idle":"2025-04-25T08:26:56.238936Z","shell.execute_reply.started":"2025-04-25T08:26:56.232461Z","shell.execute_reply":"2025-04-25T08:26:56.238217Z"},"id":"5Z2mL60HGjHy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def formal_train(json_path=\"salary_47_allin.json\", target_model_path=\"./qwen_prompt_47\"):\n","    dataset = load_and_preprocess(json_path)\n","    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"./qwen_prompt_tuning_output\",\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=2,\n","        num_train_epochs=4, # change this from 4 to 10\n","        learning_rate=3e-3, # change this from 3e-2 to 3e-3\n","        logging_dir=\"./logs\",\n","        logging_steps=100,\n","        save_strategy=\"epoch\",\n","        fp16=True,\n","        optim=\"adamw_torch\",\n","        dataloader_num_workers=4,\n","        report_to=\"none\"\n","    )\n","\n","    # 7. 创建Trainer并训练\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=dataset[\"train\"],\n","        data_collator=data_collator,\n","    )\n","\n","    print(\"开始训练...\")\n","    start_time = time.time()\n","    trainer.train()\n","    print(f\"训练完成，耗时: {time.time() - start_time:.2f}秒\")\n","\n","    model.save_pretrained(target_model_path)\n","\n","    # To check memory useage\n","    # Optional: free unused memory before checking\n","    torch.cuda.empty_cache()\n","    # Memory usage check (in GB)\n","    print(f\"Peak memory usage: {max_memory_allocated() / 1024**3:.2f} GB\")\n","\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:27:17.323759Z","iopub.execute_input":"2025-04-25T08:27:17.324067Z","iopub.status.idle":"2025-04-25T08:27:17.329755Z","shell.execute_reply.started":"2025-04-25T08:27:17.324038Z","shell.execute_reply":"2025-04-25T08:27:17.329048Z"},"id":"Te6aZllwGjHy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["formal_train(json_path=input_path, target_model_path=\"./qwen_05b_4epochs_dapt_prompt_fix_8batch_augwork\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:27:22.804796Z","iopub.execute_input":"2025-04-25T08:27:22.805508Z","iopub.status.idle":"2025-04-25T09:39:37.244539Z","shell.execute_reply.started":"2025-04-25T08:27:22.805476Z","shell.execute_reply":"2025-04-25T09:39:37.242895Z"},"colab":{"referenced_widgets":["70e32fb2b6b6441b9e7368eb40b3c9e7","904fb4d280e74a1687d0860af3b64923","f43cc0b7a6594d2d9eb89195897257d8"]},"id":"2GJHmIn4GjHy","outputId":"933fef0d-551d-4bb4-887d-b4b8a7fa8af2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70e32fb2b6b6441b9e7368eb40b3c9e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5316 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904fb4d280e74a1687d0860af3b64923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5316 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f43cc0b7a6594d2d9eb89195897257d8"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"开始训练...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2656' max='2656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2656/2656 1:11:34, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.949400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.835100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.816100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.796300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.781400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.788200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.786700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.781800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.771000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.778500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.766100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.764800</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.774000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.763900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.764200</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.763200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.770300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.741800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.758700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.764000</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.753200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.750500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.783200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.758800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.749800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.750100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"训练完成，耗时: 4297.81秒\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2393722364.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mformal_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./qwen_05b_4epochs_dapt_prompt_fix_8batch_augwork\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/4048661837.py\u001b[0m in \u001b[0;36mformal_train\u001b[0;34m(json_path, target_model_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Memory usage check (in GB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Peak memory usage: {max_memory_allocated() / 1024**3:.2f} GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'max_memory_allocated' is not defined"],"ename":"NameError","evalue":"name 'max_memory_allocated' is not defined","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":["\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","model.eval()\n","\n","def generate_response(prompt):\n","  with torch.no_grad():\n","      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","      outputs = model.generate(\n","          input_ids=inputs[\"input_ids\"],\n","          attention_mask=inputs[\"attention_mask\"],\n","          max_new_tokens=100,\n","          temperature=0.7,\n","          do_sample=True,\n","          pad_token_id=tokenizer.eos_token_id\n","      )\n","\n","      generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n","      return tokenizer.decode(generated_tokens, skip_special_tokens=True)\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:40:34.115995Z","iopub.execute_input":"2025-04-25T09:40:34.116743Z","iopub.status.idle":"2025-04-25T09:40:34.127492Z","shell.execute_reply.started":"2025-04-25T09:40:34.116715Z","shell.execute_reply":"2025-04-25T09:40:34.126759Z"},"id":"uCdO591vGjHy"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["test_path = \"/kaggle/input/job-dataset/test_1355.json\"\n","\n","# Load val_data from JSON file\n","import json\n","\n","with open(test_path, 'r') as f: #Update with the path to your JSON File.\n","    data = json.load(f)\n","\n","import time\n","\n","t0 = time.time()\n","answers = []\n","for i, item in enumerate(data):\n","  # if (0 <= i < 20) or (567 <= i < 587) or (1256 <= i):\n","    # print(\"=\" * 20, 'Round:', i, \"=\" * 20)\n","    p = item['prompt']\n","    # y = item['complete']\n","    # print(\"Target:\", y)\n","    # print(\"\\n Prompt tuning Answer:\", generate_response(p))\n","    answers.append(generate_response(p))\n","t1 = time.time()\n","print(\"Inference time:\", t1 - t0)\n","\n","import pandas as pd\n","\n","df = pd.DataFrame(data)\n","df[\"y_pred\"] = answers\n","\n","output_path = \"/kaggle/working/df_output\"\n","df.to_json(output_path, orient='records', indent=4, force_ascii=False)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:40:39.414516Z","iopub.execute_input":"2025-04-25T09:40:39.414791Z","iopub.status.idle":"2025-04-25T09:46:43.239485Z","shell.execute_reply.started":"2025-04-25T09:40:39.414770Z","shell.execute_reply":"2025-04-25T09:46:43.238849Z"},"id":"H-ZDwKZNGjHy","outputId":"dd962efa-d9fd-4835-cb82-4c1822138bb4"},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:1889: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n","output_type":"stream"},{"name":"stdout","text":"Inference time: 363.72235655784607\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"BFesNq-cGjHy"},"outputs":[],"execution_count":null}]}